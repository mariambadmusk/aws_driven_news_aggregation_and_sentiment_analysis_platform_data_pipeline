{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests \n",
    "from newsdataapi import NewsDataApiClient\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from collections import Counter\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.env']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_password = config['DB']['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise tokenizer and model for sentiment analysis\n",
    "def instantiate_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment', num_labels=3)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get news data from newsdataapi for a category\n",
    "def get_api(category):\n",
    "    try:\n",
    "        api = NewsDataApiClient(apikey = 'pub_39058ad28ae246ef9dd6ef5cbddd0efecc7ad')\n",
    "        response = api.news_api(\n",
    "                            q = category,\n",
    "                            language = 'en',\n",
    "                            image  =  False,\n",
    "                            video = False,\n",
    "                            size = 30\n",
    "                            )\n",
    "\n",
    "        print(f'News Api get request: {category} sucesss')\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f'News Api get request failed: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe from the api response for a category\n",
    "def create_dataframe(category):\n",
    "    try:\n",
    "        response = get_api(category)\n",
    "        if response:\n",
    "            df = pd.DataFrame(response['results'], columns = [\"article_id\", \"title\", \"description\", \"source\", \"link\", \"pubDate\", \"country\"])\n",
    "            return df\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f'Dataframe transformation failed: {e}')\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addcolumns for wordCount and articleLength to dataframe\n",
    "def add_count_sentiment(df):\n",
    "    df['wordCount'] = df['description'].apply(lambda x: len(x.split()))\n",
    "    df['articleLength'] = df['description'].apply(lambda x: len(x)) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform sentiment analysis on news using the tokeniser and model\n",
    "def perform_sentiment_analysis(df, tokenizer, model):\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(df, return_tensors='pt', padding=True, truncation=True)\n",
    "        results = model(tokens)\n",
    "    predicted_class = torch.argmax(results.logits, dim=1).item()\n",
    "    \n",
    "    if predicted_class == 0:\n",
    "        sentiment = \"negative\"\n",
    "    elif predicted_class == 1:\n",
    "        sentiment = \"neutral\"\n",
    "    else:\n",
    "       sentiment = \"positive\"\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add sentiment labels to to DataFrame based on api response description\n",
    "def add_sentiment_label(df, tokenizer, model):\n",
    "    try:\n",
    "        df['sentiment'] = df['description'].apply(lambda x: perform_sentiment_analysis(df, tokenizer, model))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Sentiment analysis failed: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert dataframes to sql database\n",
    "def insert_to_database(df, table_name):\n",
    "    try:\n",
    "        df.to_sql(table_name, connection, if_exists = 'append', index = False)\n",
    "        print(f'{df} appended to news_analysis_db sucessfully')\n",
    "        return df \n",
    "    except Exception as e:\n",
    "        print(f'{table_name} failed to append to news_analysis_db: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl news data, perform sentitnet analyssis, insert datatframe to dataabse\n",
    "def crawl_and_process_categories(category_name):\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('.env')\n",
    "\n",
    "    # postgres db params\n",
    "    db_params = {\n",
    "    'database': 'news_sentiment_analysis_database',\n",
    "    'host': 'localhost',\n",
    "    'user': 'postgres',\n",
    "    'password' : config['POSTGRES']['password']\n",
    "        }\n",
    "\n",
    "\n",
    "    try:\n",
    "        connection = psycopg2.connect(**db_params)\n",
    "\n",
    "        # Instantiate tokenizer and model\n",
    "        tokenizer, model = instantiate_model()\n",
    "\n",
    "        \n",
    "        #define category and table names\n",
    "    \n",
    "        category_id_mapping = {\n",
    "                                'business': 101,\n",
    "                                'crime': 102,\n",
    "                                'education': 103,\n",
    "                                'entertainment': 104,\n",
    "                                'health': 105,\n",
    "                                'science': 106\n",
    "                            }\n",
    "\n",
    "        # Fetch news data\n",
    "        response = get_api(category_name)\n",
    "\n",
    "        #Create dataframe\n",
    "        df = create_dataframe(response)\n",
    "        if df is not None:\n",
    "\n",
    "            # Perform sentiment analysis\n",
    "            df = add_sentiment_labels(df, tokenizer, model)\n",
    "\n",
    "            if df is not None:\n",
    "                #Add category ID\n",
    "                category_id = category_id_mapping[category_name]\n",
    "                df.insert(0, 'categoryID', category_id)\n",
    "\n",
    "                #add count and insert df into database\n",
    "                add_count_sentiment(df)\n",
    "                insert_to_database(df, category_name)\n",
    "        else:\n",
    "            print(f'Data in {category_name} is None')\n",
    "    except Exception as e:\n",
    "        print('An error has occured: {e}')\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    crawl_and_process_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookies_cats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
